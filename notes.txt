Notes tha are usefull for the purpose of this project



    Batch normalization

Normalizacija vhodov v vsakem layerju v mreži
Internal covariate shift (check it)
Normira vhodne podatke glede na mini batch da odšteje povprečje serije in deli s standardno deviacijo
batch normalization is usually added after convolutional layers, but before activation function

alternatives are Group Normalization, Layer Normalization, which are usefull for training with 
smaller batch sizes.
Pooling layers reduce the spatial dimensions (width, height) of the input feature map but do not 
change the number of feature map (channels), it remains the same before and after a pooling layer.

    feature calculation

#Cifar stars with 3 x 32 x 32 dimensions (channels x width x height)

# calculating in_features after conv and pool layers
# conv layer impact
# output size = (Inputsize - filter size(kernel size) + 2 * padding)/stride + 1
# default for padding and stride are 0 and 1

# 32x32 -> 28x28 as (32 - 5 + 1) = 28

# pooling layer impact
# output size = (input_size - pooling_size + 2 * padding)/stride + 1
# pooling_size = 2(2x2), padding = 0, stride = 2

# in_features = 5 x 5 x c; c = num_channels (RGB = 3, gray = 1)

    Layers fusing
Fusing layers in the context of neural networks refers to the process of combining multiple 
layers into a single layer or operation for the purpose of optimization. This technique can lead to 
significant improvements in the efficiency of a neural network, both in terms of inference speed 
and memory usage. The idea behind fusing layers is to reduce the computational overhead and memory 
footprint by minimizing the number of separate operations that need to be performed.

Convolution and Batch Normalization Fusion: A common example of layer fusion is the combination 
of a convolutional layer and a subsequent batch normalization layer. During the training phase, 
batch normalization applies scaling and shifting transformations to the output of the convolutional 
layer to standardize the inputs to the next layer. These operations can be mathematically combined 
into the convolution operation itself by adjusting the weights and biases, resulting in a single fused 
convolution layer that incorporates the effects of batch normalization. This can significantly 
reduce inference time since it eliminates the need to perform separate batch normalization computations 
during inference.

Fusing layers is primarily an optimization technique for reducing computational overhead and memory usage 
during inference, rather than a method for improving the accuracy of a model. The goal of fusing layers 
is to make a neural network run faster and more efficiently, especially on specific hardware or deployment 
environments, without affecting the model's output or accuracy.

    Residual connections

Residual connections, also known as skip connections, were introduced to help solve the problem of vanishing 
gradients in very deep neural networks. This problem occurs when gradients, during the backpropagation phase, 
become increasingly smaller as they are propagated back through the layers. This makes it difficult to train 
deep networks effectively.

A residual connection allows the output of one layer to be added to the output of a layer further ahead in 
the network. Essentially, it skips one or more layers and performs an element-wise addition operation on the 
outputs of the skipped layers and the layer where the skip ends.

The core idea behind residual connections is simple: instead of trying to learn an entirely new representation 
at each layer, the network learns modifications to the identity of the previous layer. This makes it easier 
to train very deep networks because it mitigates the vanishing gradient problem by providing a direct pathway 
for gradients to flow through.

    Attention Mechanisms

The concept of attention mechanisms in machine learning, particularly in deep learning, was inspired by the 
way humans focus on different parts of an image or sequence when processing information. In machine learning, 
attention mechanisms allow models to dynamically focus on certain parts of the input data over others, 
which is especially useful in tasks that involve sequences such as language translation or speech recognition.

In its essence, an attention mechanism weights the importance of different input parts when constructing an 
output part. For example, in sequence-to-sequence models (like those used in machine translation), the 
attention mechanism can help the model to focus on the relevant parts of the input sentence when translating 
a specific word or phrase.

Learning rate: 0.1 je precej prevelik, slabi rezultati

How to optimize hyperparameters:
1) Grid search
2) Random search
3) Bayesian optimization
4) Hyperband
5) Genetic algorithm
6) Automated Machine Learning (AutoML)

Pytorch optimizers from the most basic to more complex

SGD
SGD with Momentum
SGD with Nesterov Momentum
Adagrad
RMSprop
Adam
AdamW
NAdam
Adamax
AMSGrad

Pytorch schedulers rated from the most basic to the most complex one

StepLR
MultiStepLR
ExponentialLR
CosineAnnealingLR
ReduceLROnPlateau
CyclicLR
OneCycleLR
CosineAnnealingWarmRestarts
LambdaLR
PolynomialLR (custom implementation)

Pytorch criterions rated from the most basic one to the more advanced

MSELoss
L1Loss
CrossEntropyLoss
BCELoss
BCEWithLogitsLoss
NLLLoss
SmoothL1Loss
KLDivLoss
CosineEmbeddingLoss
TripletMarginLoss
CTCLoss
MarginRankingLoss
HingeEmbeddingLoss
MultiLabelMarginLoss

